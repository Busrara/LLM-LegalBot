# -*- coding: utf-8 -*-
"""Welcome To Colab

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/notebooks/intro.ipynb
"""

# legalbot.py

"""
LegalBot: Smart Legal Assistant
-------------------------------
- Upload legal documents (PDF)
- Ask questions about them using natural language
- LLM + LangChain + FAISS vector store
- Designed with data privacy in mind
"""

import os
import fitz  # PyMuPDF
import io
from typing import List

from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains.question_answering import load_qa_chain
from langchain.llms import OpenAI
from langchain.docstore.document import Document

# -------------------------------
# ğŸ” Set OpenAI API Key
# -------------------------------
def load_api_key():
    key = os.getenv("OPENAI_API_KEY")
    if not key:
        raise ValueError("Please set the OPENAI_API_KEY environment variable.")
    return key


# -------------------------------
# ğŸ“„ PDF Parsing (In-Memory)
# -------------------------------
def extract_text_from_pdf(file_path_or_stream) -> str:
    doc = fitz.open(stream=file_path_or_stream, filetype="pdf") if isinstance(file_path_or_stream, io.BytesIO) else fitz.open(file_path_or_stream)
    text = ""
    for page in doc:
        text += page.get_text()
    return text


# -------------------------------
# ğŸ§  Create Vector Store
# -------------------------------
def create_vectorstore_from_text(text: str):
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    docs = splitter.create_documents([text])
    embeddings = OpenAIEmbeddings()
    vectorstore = FAISS.from_documents(docs, embeddings)
    return vectorstore


# -------------------------------
# â“ Ask Questions Using LLM
# -------------------------------
def ask_question(vectorstore: FAISS, question: str) -> str:
    relevant_docs: List[Document] = vectorstore.similarity_search(question)
    llm = OpenAI(temperature=0)
    chain = load_qa_chain(llm, chain_type="stuff")
    response = chain.run(input_documents=relevant_docs, question=question)
    return response


# -------------------------------
# ğŸ§¹ Clean Memory
# -------------------------------
def clear_all():
    global vectorstore
    vectorstore = None
    print("ğŸ§¹ Vector store cleared from memory.")


# -------------------------------
# ğŸ§ª Example Usage (CLI/Test)
# -------------------------------
if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="LegalBot - Smart Legal Document QA")
    parser.add_argument("--pdf", type=str, required=True, help="Path to the PDF file")
    parser.add_argument("--question", type=str, required=True, help="Your legal question")

    args = parser.parse_args()

    print("âš–ï¸  LegalBot: Smart Legal Assistant")
    print("ğŸ” This script processes data in-memory and does not store files.")

    load_api_key()
    with open(args.pdf, "rb") as f:
        pdf_text = extract_text_from_pdf(f)

    vs = create_vectorstore_from_text(pdf_text)
    answer = ask_question(vs, args.question)

    print("\nğŸ“Œ Answer:")
    print(answer)

    clear_all()